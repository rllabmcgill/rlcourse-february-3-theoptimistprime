{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing RTDP with conventional DP methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment - Taxi Domain\n",
    "In the taxi domain, an agent navigates on a 5x5 grid with immovable objects in episodic manner. The taxi agent has to transport passengers from and to some locations predefined by the environment in this girdworld - Red, Green, Yellow and Blue. In each episode, taxi starts in a random location with a passenger waiting randomlyat one of these four locations and wishing to be dropped-off at one of the four locations chosen randomly. The taxi agent can only move in one of the four cardinal directions, pickup the passenger when in same location as the passenger and drop off the passenger. Every episode ends with the passenger dropped off at the desired location.\n",
    " \n",
    "There is a reward of -1 for every timestep. Aim is to seek a policy which maximises the total reward at the end of an episode. There are 500 different possible states the agent can be in - 25 locations on grid, 5 location of passenger and 4 drop-off spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RED = 0\n",
    "GREEN = 1\n",
    "YELLOW = 2\n",
    "BLUE = 3\n",
    "CAR = 4\n",
    "\n",
    "location = {RED:0, GREEN:4, YELLOW:20, BLUE:23}\n",
    "\n",
    "\n",
    "trans = np.array([[0,5,0,1],\n",
    "              [1,6,0,1],\n",
    "              [2,7,2,3],\n",
    "              [3,8,2,4],\n",
    "              [4,9,3,4],\n",
    "              [0,10,5,6],\n",
    "              [1,11,5,6],\n",
    "              [2,12,7,8],\n",
    "              [3,13,7,9],\n",
    "              [4,14,8,9],\n",
    "              [5,15,10,11],\n",
    "              [6,16,10,12],\n",
    "              [7,17,11,13],\n",
    "              [8,18,12,14],\n",
    "              [9,19,13,14],\n",
    "              [10,20,15,15],\n",
    "              [11,21,16,17],\n",
    "              [12,22,16,17],\n",
    "              [13,23,18,19],\n",
    "              [14,24,18,19],\n",
    "              [15,20,20,20],\n",
    "              [16,21,21,22],\n",
    "              [17,22,21,22],\n",
    "              [18,23,23,24],\n",
    "              [19,24,23,24]]).astype(\"int8\")\n",
    "\n",
    "\n",
    "class Taxi(object):\n",
    "    class observations(object):\n",
    "        def __init__(self):\n",
    "            self.car = np.random.randint(25)\n",
    "            self.passenger = np.random.choice([RED, GREEN, YELLOW, BLUE, CAR])\n",
    "            self.destination = np.random.choice([RED, GREEN, YELLOW, BLUE])\n",
    "        \n",
    "        def reset(self):\n",
    "            self.car = np.random.randint(25)\n",
    "            self.passenger = np.random.choice([RED, GREEN, YELLOW, BLUE, CAR])\n",
    "            self.destination = np.random.choice([RED, GREEN, YELLOW, BLUE])\n",
    "            return self.getFeature()    \n",
    "        \n",
    "        def getState(self):\n",
    "            return self.car, self.passenger, self.destination\n",
    "        \n",
    "        def getFeature(self):\n",
    "            return self.destination + self.passenger*4 + self.car*5*4\n",
    "            \n",
    "        def setState(self, feature):\n",
    "            self.car = feature/20\n",
    "            feature %= 20\n",
    "            self.passenger = feature/4\n",
    "            feature %= 4\n",
    "            self.destination = feature \n",
    "\n",
    "        def sample(self):\n",
    "            return self.reset()\n",
    "\n",
    "    def __init__(self):\n",
    "        self.trans = trans\n",
    "        self.observation_space = self.observations()\n",
    "        self.acc_reward = 0\n",
    "        self.done = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.acc_reward = 0\n",
    "        self.done = False\n",
    "        return self.observation_space.reset()\n",
    "\n",
    "    def totalReward(self):\n",
    "        return self.acc_reward\n",
    "\n",
    "    def goalState(self):\n",
    "        if self.observation_space.passenger == CAR:\n",
    "            return location[self.observation_space.destination]\n",
    "        else:\n",
    "            return location[self.observation_space.passenger]\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = -1\n",
    "        if action<4:\n",
    "            self.observation_space.car = self.trans[self.observation_space.car,action]\n",
    "            \"\"\"if self.picked:\n",
    "                self.observation_space.passenger = self.observation_space.car\"\"\"\n",
    "        elif action == 4:\n",
    "            if self.observation_space.passenger!=CAR and self.observation_space.car == location[self.observation_space.passenger]:\n",
    "                reward = 0\n",
    "                self.observation_space.passenger = CAR\n",
    "        elif action == 5:\n",
    "            if self.observation_space.car==location[self.observation_space.destination] and self.observation_space.passenger==CAR:\n",
    "                reward = 1\n",
    "                self.done = True\n",
    "        self.acc_reward += reward\n",
    "        return self.observation_space.getFeature(), reward, self.done\n",
    "\n",
    "    def episodeEnded(self):\n",
    "        return self.done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learning reward function\n",
    "demoT = Taxi()\n",
    "r = np.zeros((500,6))\n",
    "t = np.zeros((500,6))\n",
    "\n",
    "for state in range(500):\n",
    "    for action in range(6):\n",
    "        demoT.observation_space.setState(state)\n",
    "        t[state, action], r[state, action], _ = demoT.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r[s,a] = Reward received when action 'a' is performed in state s\n",
    "\n",
    "t[s,a] = Taxi transitions to this state after excution of action a in state s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "gamma = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility Functions -\n",
    "to check convergence to optimal policy. As the MDP defined above can experience vastly different costs for starting in different states, average cost on a small number of random restarts is not a good way to test convergence to optimal policy. As a work around this problem, I'll be comparing the costs to those of certain fixed starting states with a greedy policy over V*. (I use stationary, $\\delta \\lt 10^{-7}$, approximate Value Function from Value Iteration) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vStar = None\n",
    "testingStates = None\n",
    "\n",
    "def idealRewardFromRun(env, states):\n",
    "    global vStar\n",
    "    if vStar==None:\n",
    "        vStar=VI()\n",
    "    idealRewards={}\n",
    "\n",
    "    for st in states:\n",
    "        env.reset()\n",
    "        env.observation_space.setState(st)\n",
    "        \n",
    "        s = st\n",
    "        done = False\n",
    "        while not done:\n",
    "            q=np.zeros(6)\n",
    "            for a in range(6):\n",
    "                q[a] = (r[s,a]+gamma*vStar[t[s,a]])\n",
    "            action =np.argmax(q)\n",
    "            s,_, done = env.step(action)\n",
    "        idealRewards[st] = env.totalReward()\n",
    "    return idealRewards \n",
    "\n",
    "def testConvergence(env, valueFunc, margin=3):\n",
    "    global testingStates\n",
    "    if testingStates==None:\n",
    "        testingStates = idealRewardFromRun(env, [158])\n",
    "        print \"testingStates generated!\"\n",
    "        \n",
    "    for st in testingStates.keys():\n",
    "        epoch=0\n",
    "        env.reset()\n",
    "        env.observation_space.setState(st)\n",
    "        lim = int((abs(testingStates[st])+margin)*1.1)\n",
    "        s=st\n",
    "        done=False\n",
    "        while (not done) and epoch<lim:\n",
    "            epoch+=1\n",
    "            val = [(a, r[s,a]+gamma*valueFunc[t[s,a]]) for a in range(6)]\n",
    "            np.random.shuffle(val)\n",
    "            action = max(val, key=lambda pair:pair[1])[0]\n",
    "            s,_,done = env.step(action)\n",
    "        if epoch==lim:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def VI(eps = 0.0000001):\n",
    "    v = np.zeros(500)\n",
    "    delta = np.inf\n",
    "    sweep = 0\n",
    "    \n",
    "    while(delta>eps and sweep<1000):\n",
    "        sweep +=1\n",
    "        delta=0\n",
    "        v_old = np.copy(v)\n",
    "\n",
    "        for state in range(500):\n",
    "            temp=[]\n",
    "            for action in range(6):\n",
    "                temp.append(r[state, action] + gamma*v_old[t[state, action]])\n",
    "                \n",
    "            v[state] = max(temp)\n",
    "            delta = max(delta, abs(v[state] - v_old[state]))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:61: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:19: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{320: -3, 385: -2, 261: -12, 262: -8, 70: -6, 272: -8, 19: -6, 358: -4, 92: -11, 29: -12}\n",
      "testingStates generated!\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:6: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:40: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# testing Convergence utilities -should't converge using a random policy\n",
    "print idealRewardFromRun(Taxi(),np.random.randint(0,500, 10))\n",
    "print testConvergence(Taxi(), np.random.random(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Synchronous dynamic programming \n",
    "I use Value Iteration to show results.\n",
    "We can think about this case fo DP as one where updates to all the states are applied at the same time. Therefore, an update for $V_{k+1}$ uses estimates $V_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ValueIteration(eps = 0.0000001):\n",
    "    v = np.zeros(500)\n",
    "    delta = np.inf\n",
    "    converged = False\n",
    "    sweep = 0\n",
    "    updates_opt_policy = 0\n",
    "    updates = 0\n",
    "    while(delta>eps and sweep<1000):\n",
    "        sweep +=1\n",
    "        delta=0\n",
    "        v_old = np.copy(v)\n",
    "\n",
    "        for state in range(500):\n",
    "            updates+=1\n",
    "            temp=[]\n",
    "            for action in range(6):\n",
    "                temp.append(r[state, action] + gamma*v_old[t[state, action]])\n",
    "                \n",
    "            v[state] = max(temp)\n",
    "            delta = max(delta, abs(v[state] - v_old[state]))\n",
    "            if not converged:\n",
    "                converged = testConvergence(Taxi(), v)\n",
    "                updates_opt_policy = updates\n",
    "    return v, sweep, updates, updates_opt_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss-Seidel dynamic programming\n",
    "It is a type of Asyncronous DP. Gauss-Seidel DP differs from the synchronous version in that the costs are backed\n",
    "up one state at a time in a sequential “sweep” of all the states, with the computation for each state using the most recent costs of the other states. If we assume that the states are numbered in order, as we have here, and that each sweep proceeds in this order, then the result of each iteration of Gauss-Seidel DP can be written as follows: \n",
    "\n",
    "$ \\forall$ states $s_i$ and k = 0, 1, ... \n",
    "\n",
    "$ V_{k+1}(s_i)$ = $max_a$ [ r($s_i$,a) + $\\gamma$V($s_j$) ];\n",
    "\n",
    "where $V(s_j)$ = $ V_{k+1}(s_j) $ if j$\\lt$i, else $ V_{k}(s_j) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def GaussSeidelValueIteration(eps=0.0000001):\n",
    "    v = np.zeros(500)\n",
    "    delta = np.inf\n",
    "    sweep = 0\n",
    "    updates = 0\n",
    "    updates_opt_policy = 0\n",
    "    converged = False\n",
    "    \n",
    "    while(delta>eps and sweep<1000):\n",
    "        sweep +=1\n",
    "        delta=0\n",
    "        v_old = np.copy(v)\n",
    "\n",
    "        for state in range(500):\n",
    "            temp=[]\n",
    "            for action in range(6):\n",
    "                temp.append(r[state, action] + gamma*v[t[state, action]])\n",
    "            v[state] = max(temp)\n",
    "            updates+=1\n",
    "            \n",
    "            delta = max(delta, abs(v[state] - v_old[state]))\n",
    "        if not converged:\n",
    "            converged = testConvergence(Taxi(), v)\n",
    "            updates_opt_policy = updates   \n",
    "    return v, sweep, updates, updates_opt_policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-Time Dynamic Programming\n",
    "The Gauss-Seidel DP algorithm implement above is off-line for solving Markovian decision problems. Although they successively approximate the optimal evaluation function through a sequence of stages, these stages are not related to the time steps of the decision problem being solved. RLDP we consider here performs asynchronous DP concurrently with the actual process of control. \n",
    "\n",
    "RTDP refers to cases in which the concurrently executing DP and controller influence one another as follows:\n",
    "\n",
    "First, the controller always follows a greedy policy w.r.t. the most recent estimate of $V^{*}$. This means that $a_t$ is always the greedy action with respect to $V_{k_t}$. Any ties in selecting these actions is resolved randomly, which ensures the continuing selection of all the greedy actions. \n",
    "\n",
    "Second, between the execution of $a_t$, and $a_{t+1}$, the cost of s, is always backed up, but more generally, any state can also be updates in this trial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RLDP\n",
      "converged within 6 epochs with just 339 updates.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:13: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:40: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "def RLDP(env, eps=0.0000001):\n",
    "    v = 0.5*np.zeros(500)\n",
    "    epoch = 0\n",
    "    converged = False\n",
    "    updates=0\n",
    "    while(not converged and epoch<1000):\n",
    "        epoch +=1\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            q=np.zeros(6)\n",
    "            for a in range(6):\n",
    "                q[a] = (r[s,a]+gamma*v[t[s,a]])\n",
    "            action =np.argmax(q)\n",
    "            v[s] = np.max(q)\n",
    "            updates+=1\n",
    "            s, _, done = env.step(action)\n",
    "        \n",
    "        converged = testConvergence(env, v)\n",
    "    \n",
    "    return v, epoch, updates\n",
    "\n",
    "_, epochs, totalUpdates = RLDP(Taxi())\n",
    "print \"RLDP\"\n",
    "print \"converged within\", epochs,\"epochs with just\",totalUpdates,\"updates.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:17: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:40: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged to optimal values in 101 sweeps with total 50500\n",
      "Convergence for test starting states was reached with 1046 updates\n"
     ]
    }
   ],
   "source": [
    "print \"Value Iteration\"\n",
    "_, sweeps, totalUpdates, UpdatesForConvergence = ValueIteration()\n",
    "print \"converged to optimal values in\",sweeps,\"sweeps with total\",totalUpdates\n",
    "print \"Convergence for test starting states was reached with\",UpdatesForConvergence,\"updates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gauss-Seidel Value Iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:17: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:40: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged to optimal values in 101 sweeps with total 50500\n",
      "Convergence for test starting states was reached with 2000 updates\n"
     ]
    }
   ],
   "source": [
    "print \"Gauss-Seidel Value Iteration\"\n",
    "_, sweeps, totalUpdates, UpdatesForConvergence = GaussSeidelValueIteration()\n",
    "print \"converged to optimal values in\",sweeps,\"sweeps with total\",totalUpdates\n",
    "print \"Convergence for test starting states was reached with\",UpdatesForConvergence,\"updates\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, RLDP converges to optimal values for relevant states much fast (lesser totat updates for cenvergence) compared to classical DP methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
